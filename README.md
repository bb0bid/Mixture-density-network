# Mixture-density-network
Mixture Density Networks were first introduced by Christopher M. Bishop in 1994. In conventional neural networks, the expected conditional probability of the target conditioned on input is estimated by minimizing the sum of squared residuals or the cross-entropy error function. Also the conditional probability distribution is assumed to be a Gaussian distribution with zero mean and variance independent of input. A conventional neural network learns this probability distribution and predicts the mean of the distribution for each input training example. However, in many real world scenarios, the target data cannot be fully described using just the conditional averages. Especially, when the underlying function that our model is learning is a multivalued function which is generally the case for inverse functions. In this case, the target variable can be thought of as being sampled a mixture of probability distribution instead of a single distribution. Mixture Density Networks help in predicting such distributions.
